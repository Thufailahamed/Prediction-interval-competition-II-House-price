{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":99650,"databundleVersionId":11917221,"sourceType":"competition"}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-15T12:25:58.155966Z","iopub.execute_input":"2025-07-15T12:25:58.156562Z","iopub.status.idle":"2025-07-15T12:26:00.067779Z","shell.execute_reply.started":"2025-07-15T12:25:58.156532Z","shell.execute_reply":"2025-07-15T12:26:00.067014Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/prediction-interval-competition-ii-house-price/sample_submission.csv\n/kaggle/input/prediction-interval-competition-ii-house-price/test.csv\n/kaggle/input/prediction-interval-competition-ii-house-price/dataset.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install mapie -qq","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T12:26:00.068551Z","iopub.execute_input":"2025-07-15T12:26:00.068901Z","iopub.status.idle":"2025-07-15T12:26:07.879074Z","shell.execute_reply.started":"2025-07-15T12:26:00.068883Z","shell.execute_reply":"2025-07-15T12:26:07.878208Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m91.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncategory-encoders 2.7.0 requires scikit-learn<1.6.0,>=1.0.0, but you have scikit-learn 1.7.0 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nsklearn-compat 0.1.3 requires scikit-learn<1.7,>=1.2, but you have scikit-learn 1.7.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# ===================================================================\n#   CQR Baseline with CV+: LightGBM with mapie (prefit=True)\n# ===================================================================\n\n# --- 0. Import Libraries ---\nimport pandas as pd\nimport numpy as np\nimport lightgbm as lgb\nfrom sklearn.model_selection import KFold\nfrom mapie.regression import ConformalizedQuantileRegressor\nimport warnings\nimport os\n\nwarnings.filterwarnings('ignore')\nprint(\"Starting: CQR_LGBM Baseline with prefit=True\")\n\n# --- 1. Configuration and Utility Functions ---\nclass CFG:\n    # Global settings\n    SEED = 42\n    N_SPLITS = 5  # Number of folds for cross-validation\n    CONFIDENCE_LEVEL = 0.9  # Target confidence level (90%)\n    ALPHA = 1 - CONFIDENCE_LEVEL\n\n    # File paths\n    try:\n        # Kaggle environment\n        DATA_PATH = '/kaggle/input/prediction-interval-competition-ii-house-price/'\n        if not os.path.exists(os.path.join(DATA_PATH, 'dataset.csv')):\n            raise FileNotFoundError\n    except FileNotFoundError:\n        print(\"Kaggle path not found or files missing, switching to local path './'.\")\n        DATA_PATH = './'\n    OUTPUT_PATH = './'\n\n    # Base parameters for the quantile models\n    LGBM_PARAMS = {\n        'objective': 'quantile',\n        'metric': 'quantile',\n        'n_estimators': 2000, # Increased for early stopping\n        'subsample': 0.8,\n        'colsample_bytree': 0.5,\n        'learning_rate': 0.05,\n        'max_depth': -1,\n        'min_child_samples': 150,\n        'n_jobs': -1,\n        'random_state': SEED,\n        'verbose': -1,\n    }\n\ndef winkler_score_func(y_true, lower, upper, alpha=CFG.ALPHA):\n    \"\"\"Utility function to calculate the Winkler score.\"\"\"\n    score = np.mean(upper - lower)\n    score += np.mean(np.where(y_true < lower, (2 / alpha) * (lower - y_true), 0))\n    score += np.mean(np.where(y_true > upper, (2 / alpha) * (y_true - upper), 0))\n    return score\n\n# --- 2. Data Loading and Preprocessing ---\nprint(\"\\n--- Phase 1: Loading and Preprocessing Data ---\")\ntry:\n    train_df_raw = pd.read_csv(os.path.join(CFG.DATA_PATH, 'dataset.csv'))\n    test_df_raw = pd.read_csv(os.path.join(CFG.DATA_PATH, 'test.csv'))\nexcept FileNotFoundError:\n    print(\"Error: dataset.csv or test.csv not found.\")\n    print(\"Creating dummy data for demonstration purposes.\")\n    train_df_raw = pd.DataFrame({\n        'id': range(1000), 'sale_price': np.random.rand(1000) * 500000 + 100000,\n        'sale_date': pd.to_datetime(pd.date_range(start='2022-01-01', periods=1000)),\n        'category_feature': np.random.choice(['A', 'B', 'C'], 1000)\n    })\n    test_df_raw = pd.DataFrame({\n        'id': range(1000, 1200),\n        'sale_date': pd.to_datetime(pd.date_range(start='2024-09-01', periods=200)),\n        'category_feature': np.random.choice(['A', 'B', 'C'], 200)\n    })\n\ndef feature_engineer(df):\n    \"\"\"Simple feature engineering.\"\"\"\n    data = df.copy()\n    if 'sale_date' in data.columns:\n        data['sale_date'] = pd.to_datetime(data['sale_date'])\n        data['sale_year'] = data['sale_date'].dt.year\n        data['sale_month'] = data['sale_date'].dt.month\n        data['sale_dayofweek'] = data['sale_date'].dt.dayofweek\n        first_sale_month = data['sale_date'].dt.to_period('M').min()\n        data['months_since_first_sale'] = (data['sale_date'].dt.to_period('M') - first_sale_month).apply(lambda x: x.n)\n        data = data.drop('sale_date', axis=1)\n    cat_cols = data.select_dtypes(include=['object']).columns\n    for col in cat_cols:\n        data[col] = pd.Categorical(data[col])\n    return data\n\ntrain_df = feature_engineer(train_df_raw)\ntest_df_processed = feature_engineer(test_df_raw)\n\nfeatures = [col for col in train_df.columns if col not in ['id', 'sale_price']]\ny = train_df['sale_price']\nX = train_df[features]\nX_test = test_df_processed[features]\n\nprint(f\"Training with {len(features)} features. Train shape: {X.shape}, Test shape: {X_test.shape}\")\n\n# --- 3. Cross-Validation Training with prefit=True ---\nprint(f\"\\n--- Phase 2: Training with {CFG.N_SPLITS}-Fold CV and prefit CQR ---\")\nkf = KFold(n_splits=CFG.N_SPLITS, shuffle=True, random_state=CFG.SEED)\n\noof_preds_lower = np.zeros(len(train_df))\noof_preds_upper = np.zeros(len(train_df))\ntest_preds_lower_sum = np.zeros(len(test_df_raw))\ntest_preds_upper_sum = np.zeros(len(test_df_raw))\nfold_scores = []\n\n# Define parameters for the three quantile models\nparams_lower = {**CFG.LGBM_PARAMS, 'alpha': CFG.ALPHA / 2}\nparams_median = {**CFG.LGBM_PARAMS, 'alpha': 0.5}\nparams_upper = {**CFG.LGBM_PARAMS, 'alpha': 1 - (CFG.ALPHA / 2)}\n\nfor fold, (fit_idx, calib_idx) in enumerate(kf.split(X, y)):\n    print(f\"\\n--- Fold {fold+1}/{CFG.N_SPLITS} ---\")\n    X_fit, X_calib = X.iloc[fit_idx], X.iloc[calib_idx]\n    y_fit, y_calib = y.iloc[fit_idx], y.iloc[calib_idx]\n\n    # Step 1: Fit the three quantile models on the fitting dataset\n    print(\"Fitting lower, median, and upper models...\")\n    model_lower = lgb.LGBMRegressor(**params_lower)\n    model_median = lgb.LGBMRegressor(**params_median)\n    model_upper = lgb.LGBMRegressor(**params_upper)\n\n    callbacks = [lgb.early_stopping(100, verbose=False)]\n    model_lower.fit(X_fit, y_fit, eval_set=[(X_calib, y_calib)], callbacks=callbacks)\n    model_median.fit(X_fit, y_fit, eval_set=[(X_calib, y_calib)], callbacks=callbacks)\n    model_upper.fit(X_fit, y_fit, eval_set=[(X_calib, y_calib)], callbacks=callbacks)\n\n    # Step 2: Conformalize using the pre-fitted models and the calibration dataset\n    print(\"Conformalizing models...\")\n    mapie_cqr = ConformalizedQuantileRegressor(\n        estimator=[model_lower, model_upper, model_median], # [lower, upper, median] order\n        confidence_level=CFG.CONFIDENCE_LEVEL,\n        prefit=True\n    ).conformalize(X_calib, y_calib)\n\n    # Step 3: Generate OOF predictions for the calibration set\n    _, oof_pis = mapie_cqr.predict_interval(X_calib)\n    oof_preds_lower[calib_idx] = oof_pis[:, 0, 0]\n    oof_preds_upper[calib_idx] = oof_pis[:, 1, 0]\n\n    fold_score = winkler_score_func(y_calib, oof_pis[:, 0, 0], oof_pis[:, 1, 0])\n    fold_scores.append(fold_score)\n    print(f\"Fold {fold+1} Winkler Score: {fold_score:,.2f}\")\n\n    # Step 4: Generate predictions for the test set and accumulate them\n    print(\"Predicting on test data...\")\n    _, test_pis = mapie_cqr.predict_interval(X_test)\n    test_preds_lower_sum += test_pis[:, 0, 0]\n    test_preds_upper_sum += test_pis[:, 1, 0]\n\n# --- 4. Final Evaluation and Submission ---\nprint(\"\\n--- Phase 3: Final Evaluation and Submission ---\")\n\noverall_oof_score = winkler_score_func(y, oof_preds_lower, oof_preds_upper)\nprint(f\"\\nFold Scores: {[f'{s:,.2f}' for s in fold_scores]}\")\nprint(f\"Overall OOF Winkler Score: {overall_oof_score:,.2f}\")\n\ntest_preds_lower = test_preds_lower_sum / CFG.N_SPLITS\ntest_preds_upper = test_preds_upper_sum / CFG.N_SPLITS\n\nsubmission_df = pd.DataFrame({\n    'id': test_df_raw['id'],\n    'pi_lower': test_preds_lower,\n    'pi_upper': test_preds_upper\n})\nsubmission_df['pi_lower'] = np.minimum(submission_df['pi_lower'], submission_df['pi_upper'])\nsubmission_df.to_csv(os.path.join(CFG.OUTPUT_PATH, 'submission_baseline_cqr_prefit.csv'), index=False)\n\nprint(\"\\nSubmission file 'submission_baseline_cqr_prefit.csv' has been created.\")\nprint(submission_df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T12:26:07.880911Z","iopub.execute_input":"2025-07-15T12:26:07.881170Z","iopub.status.idle":"2025-07-15T12:48:54.350862Z","shell.execute_reply.started":"2025-07-15T12:26:07.881129Z","shell.execute_reply":"2025-07-15T12:48:54.350063Z"}},"outputs":[{"name":"stdout","text":"Starting: CQR_LGBM Baseline with prefit=True\n\n--- Phase 1: Loading and Preprocessing Data ---\nTraining with 48 features. Train shape: (200000, 48), Test shape: (200000, 48)\n\n--- Phase 2: Training with 5-Fold CV and prefit CQR ---\n\n--- Fold 1/5 ---\nFitting lower, median, and upper models...\nConformalizing models...\nFold 1 Winkler Score: 341,002.37\nPredicting on test data...\n\n--- Fold 2/5 ---\nFitting lower, median, and upper models...\nConformalizing models...\nFold 2 Winkler Score: 339,799.96\nPredicting on test data...\n\n--- Fold 3/5 ---\nFitting lower, median, and upper models...\nConformalizing models...\nFold 3 Winkler Score: 344,248.76\nPredicting on test data...\n\n--- Fold 4/5 ---\nFitting lower, median, and upper models...\nConformalizing models...\nFold 4 Winkler Score: 336,733.19\nPredicting on test data...\n\n--- Fold 5/5 ---\nFitting lower, median, and upper models...\nConformalizing models...\nFold 5 Winkler Score: 343,948.48\nPredicting on test data...\n\n--- Phase 3: Final Evaluation and Submission ---\n\nFold Scores: ['341,002.37', '339,799.96', '344,248.76', '336,733.19', '343,948.48']\nOverall OOF Winkler Score: 341,146.55\n\nSubmission file 'submission_baseline_cqr_prefit.csv' has been created.\n       id       pi_lower      pi_upper\n0  200000  794127.889009  1.072542e+06\n1  200001  536152.099540  7.511893e+05\n2  200002  454642.119007  6.777121e+05\n3  200003  299398.364681  4.182662e+05\n4  200004  376845.271308  7.728627e+05\n","output_type":"stream"}],"execution_count":3}]}